<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">Front. Neurosci.</journal-id>
<journal-title>Frontiers in Neuroscience</journal-title>
<abbrev-journal-title abbrev-type="pubmed">Front. Neurosci.</abbrev-journal-title>
<issn pub-type="epub">1662-453X</issn>
<publisher>
<publisher-name>Frontiers Media S.A.</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3389/fnins.2014.00228</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Psychology</subject>
<subj-group>
<subject>Original Research Article</subject>
</subj-group>
</subj-group>
</article-categories>
<title-group>
<article-title>Gender differences in the temporal voice areas</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Ahrens</surname> <given-names>Merle-Marie</given-names></name>
<xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
<xref ref-type="author-notes" rid="fn001"><sup>&#x0002A;</sup></xref>
<uri xlink:href="http://community.frontiersin.org/people/u/126908"/>
</contrib>
<contrib contrib-type="author">
<name><surname>Awwad Shiekh Hasan</surname> <given-names>Bashar</given-names></name>
<xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<uri xlink:href="http://community.frontiersin.org/people/u/127003"/>
</contrib>
<contrib contrib-type="author">
<name><surname>Giordano</surname> <given-names>Bruno L.</given-names></name>
<xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<uri xlink:href="http://community.frontiersin.org/people/u/31258"/>
</contrib>
<contrib contrib-type="author">
<name><surname>Belin</surname> <given-names>Pascal</given-names></name>
<xref ref-type="aff" rid="aff1"><sup>1</sup></xref>
<xref ref-type="aff" rid="aff2"><sup>2</sup></xref>
<xref ref-type="aff" rid="aff3"><sup>3</sup></xref>
<uri xlink:href="http://community.frontiersin.org/people/u/47739"/>
</contrib>
</contrib-group>
<aff id="aff1"><sup>1</sup><institution>Centre for Cognitive Neuroimaging, Institute of Neuroscience and Psychology, University of Glasgow</institution> <country>Glasgow, UK</country></aff>
<aff id="aff2"><sup>2</sup><institution>Institut des Neurosciences de la Timone, UMR 7289, CNRS and Universit&#x000E9; Aix-Marseille</institution> <country>Marseille, France</country></aff>
<aff id="aff3"><sup>3</sup><institution>International Laboratories for Brain, Music and Sound, Department of Psychology, Universit&#x000E9; de Montr&#x000E9;al, McGill University</institution> <country>Montreal, QC, Canada</country></aff>
<author-notes>
<fn fn-type="edited-by"><p>Edited by: Micah M. Murray, University Hospital Center and University of Lausanne, Switzerland</p></fn>
<fn fn-type="edited-by"><p>Reviewed by: Milene Bonte, Maastricht University, Netherlands; Olivier Joly, MRC Cognition and Brain Sciences Unit, UK</p></fn>
<fn fn-type="corresp" id="fn001"><p>&#x0002A;Correspondence: Merle-Marie Ahrens, Institute of Neuroscience and Psychology, University of Glasgow, 58 Hillhead Street, Glasgow G12 8QB, UK e-mail: <email>merlea&#x00040;psy.gla.ac.uk</email></p></fn>
<fn fn-type="other" id="fn002"><p>This article was submitted to Auditory Cognitive Neuroscience, a section of the journal Frontiers in Neuroscience.</p></fn>
</author-notes>
<pub-date pub-type="epub">
<day>30</day>
<month>07</month>
<year>2014</year>
</pub-date>
<pub-date pub-type="collection">
<year>2014</year>
</pub-date>
<volume>8</volume>
<elocation-id>228</elocation-id>
<history>
<date date-type="received">
<day>09</day>
<month>12</month>
<year>2013</year>
</date>
<date date-type="accepted">
<day>10</day>
<month>07</month>
<year>2014</year>
</date>
</history>
<permissions>
<copyright-statement>Copyright &#x000A9; 2014 Ahrens, Awwad Shiekh Hasan, Giordano and Belin.</copyright-statement>
<copyright-year>2014</copyright-year>
<license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/3.0/"><p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) or licensor are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</p>
</license>
</permissions>
<abstract><p>There is not only evidence for behavioral differences in voice perception between female and male listeners, but also recent suggestions for differences in neural correlates between genders. The fMRI functional voice localizer (comprising a univariate analysis contrasting stimulation with vocal vs. non-vocal sounds) is known to give robust estimates of the temporal voice areas (TVAs). However, there is growing interest in employing multivariate analysis approaches to fMRI data (e.g., multivariate pattern analysis; MVPA). The aim of the current study was to localize voice-related areas in both female and male listeners and to investigate whether brain maps may differ depending on the gender of the listener. After a univariate analysis, a random effects analysis was performed on female (<italic>n</italic> &#x0003D; 149) and male (<italic>n</italic> &#x0003D; 123) listeners and contrasts between them were computed. In addition, MVPA with a whole-brain searchlight approach was implemented and classification maps were entered into a second-level permutation based random effects models using statistical non-parametric mapping (SnPM; Nichols and Holmes, <xref ref-type="bibr" rid="B25">2002</xref>). Gender differences were found only in the MVPA. Identified regions were located in the middle part of the middle temporal gyrus (bilateral) and the middle superior temporal gyrus (right hemisphere). Our results suggest differences in classifier performance between genders in response to the voice localizer with higher classification accuracy from local BOLD signal patterns in several temporal-lobe regions in female listeners.</p></abstract>
<kwd-group>
<kwd>gender difference</kwd>
<kwd>fMRI</kwd>
<kwd>voice localizer</kwd>
<kwd>temporal voice areas</kwd>
<kwd>multivariate pattern analysis (MVPA)</kwd>
<kwd>voice perception</kwd>
</kwd-group>
<counts>
<fig-count count="2"/>
<table-count count="3"/>
<equation-count count="0"/>
<ref-count count="38"/>
<page-count count="8"/>
<word-count count="6691"/>
</counts>
</article-meta>
</front>
<body>
<sec sec-type="introduction" id="s1">
<title>Introduction</title>
<p>Prior functional magnetic resonance imaging (fMRI) findings suggest a robust brain response to vocal vs. non-vocal sounds in many regions of the human auditory cortex in particular in the superior temporal gyrus (STG). Vocal sounds, including but not restricted to speech sounds, evoke a greater response than non-vocal sounds with bilateral activation foci located near the anterior part of the STG extending to anterior parts of the superior temporal sulcus (STS) and posterior foci located in the middle STS (Binder et al., <xref ref-type="bibr" rid="B6">2000</xref>; Belin et al., <xref ref-type="bibr" rid="B5">2000</xref>, <xref ref-type="bibr" rid="B4">2002</xref>). Using the functional voice localizer, these findings were replicated and used in various studies (Belin et al., <xref ref-type="bibr" rid="B5">2000</xref>, <xref ref-type="bibr" rid="B4">2002</xref>; Kreifelts et al., <xref ref-type="bibr" rid="B21">2009</xref>; Latinus et al., <xref ref-type="bibr" rid="B23">2011</xref>; Ethofer et al., <xref ref-type="bibr" rid="B10">2012</xref>). The conventional way of identifying voice sensitive regions is by applying univariate statistics, implemented using a Generalized-Linear Model (GLM), to fMRI data assuming independence among voxels.</p>
<p>Interest has recently grown in applying multivariate approaches (e.g., Multivariate pattern analysis; MVPA). Instead of modeling individual voxels independently (univariate analysis), MVPA considers the information of distributed pattern in several voxels (e.g., Norman et al., <xref ref-type="bibr" rid="B26">2006</xref>; Mur et al., <xref ref-type="bibr" rid="B24">2009</xref>). Several studies used multivariate approaches to decode information reflected in brain activity patterns related to specific experimental conditions (Cox and Savoy, <xref ref-type="bibr" rid="B8">2003</xref>; Haynes and Rees, <xref ref-type="bibr" rid="B14">2005</xref>, <xref ref-type="bibr" rid="B15">2006</xref>; Kotz et al., <xref ref-type="bibr" rid="B20">2013</xref>). MVPA is usually applied on unsmoothed data preserving high spatial frequency information. Thus, MVPA is argued to be more sensitive in detecting different cognitive states. In contrast, the conventional univariate analysis averages across voxels, thereby removing focally distributed effects (spatial smoothing). The smoothing across voxels may lead to a reduction in the information content (Kriegeskorte et al., <xref ref-type="bibr" rid="B22">2006</xref>; Norman et al., <xref ref-type="bibr" rid="B26">2006</xref>; Haynes et al., <xref ref-type="bibr" rid="B16">2007</xref>). At present, a multivariate approach has never been employed to investigate whether it may yield a different pattern of voice-specific (voice/non-voice classification) brain regions compared to the univariate analysis.</p>
<p>The voice contains socially and biologically relevant information and plays a crucial role in human interaction. This information is particularly relevant for interaction between different genders (e.g., regarding emotions, identities, and attractiveness) (Belin et al., <xref ref-type="bibr" rid="B2">2004</xref>, <xref ref-type="bibr" rid="B1">2011</xref>). Overall, research suggests that women are more sensitive than men in emotion recognition from faces and voices (Hall, <xref ref-type="bibr" rid="B12">1978</xref>; Hall et al., <xref ref-type="bibr" rid="B13">2006</xref>; Schirmer and Kotz, <xref ref-type="bibr" rid="B28">2006</xref>). Women perform better in judging others&#x00027; non-verbal behavior (Hall, <xref ref-type="bibr" rid="B12">1978</xref>) and seem to process nonverbal emotional information more automatically as compared to men (Schirmer et al., <xref ref-type="bibr" rid="B31">2005</xref>). In addition, women but not men show greater limbic activity when processing emotional facial expressions (Hall et al., <xref ref-type="bibr" rid="B11">2004</xref>). The exact neural mechanisms underlying voice processing in both female and male listeners still remains under debate. For instance, a study by Lattner et al. (<xref ref-type="bibr" rid="B23a">2005</xref>) found no significant difference between the activation patterns of female and male listeners in response to voice-related information. However, there is evidence from both behavioral and neural activation studies for differences in voice perception between listeners&#x00027; gender (Shaywitz et al., <xref ref-type="bibr" rid="B35">1995</xref>; Schirmer et al., <xref ref-type="bibr" rid="B29">2002</xref>, <xref ref-type="bibr" rid="B32">2004</xref>, <xref ref-type="bibr" rid="B30">2007</xref>; Junger et al., <xref ref-type="bibr" rid="B19">2013</xref>; Skuk and Schweinberger, <xref ref-type="bibr" rid="B36">2013</xref>).</p>
<p>A recent behavioral study by Skuk and Schweinberger (<xref ref-type="bibr" rid="B36">2013</xref>) investigated gender differences in a familiar voice identification task. They found an own-gender bias for males but not for females while females outperformed males overall. These behavioral differences (Skuk and Schweinberger, <xref ref-type="bibr" rid="B36">2013</xref>) may also be reflected by differences in neural activity. Previous fMRI studies investigating potential neural correlates suggested a sex difference in the functional organization of the brain for phonological processing (Shaywitz et al., <xref ref-type="bibr" rid="B35">1995</xref>), in emotional prosodic and semantic processing (Schirmer et al., <xref ref-type="bibr" rid="B29">2002</xref>, <xref ref-type="bibr" rid="B32">2004</xref>) and in response to gender-specific voice perception (Junger et al., <xref ref-type="bibr" rid="B19">2013</xref>). Further evidence suggests differences between genders in vocal processing shown by an EEG study, where the processing of vocal sounds with more emotional and/or social information was more sensitive in women as compared to men (Schirmer and Kotz, <xref ref-type="bibr" rid="B28">2006</xref>; Schirmer et al., <xref ref-type="bibr" rid="B30">2007</xref>). The above-mentioned studies mainly focus on gender differences in emotional speech processing or opposite-sex perception. However, identified brain regions are not consistent: different experimental designs and applied methods vary and make it difficult to compare between these studies (Shaywitz et al., <xref ref-type="bibr" rid="B35">1995</xref>; Schirmer et al., <xref ref-type="bibr" rid="B29">2002</xref>, <xref ref-type="bibr" rid="B32">2004</xref>, <xref ref-type="bibr" rid="B30">2007</xref>; Junger et al., <xref ref-type="bibr" rid="B19">2013</xref>).</p>
<p>The current study employs a well-established experimental design of the functional &#x0201C;voice localizer,&#x0201D; known to give robust estimates of the TVAs across the majority of participants. The voice localizer includes a variety of different vocal sounds, not exclusively female or male voices, but also speech and non-speech of women, men and infants and non-vocal sounds (e.g., environmental sounds). In this study, we were interested in the effect of gender on the results of the voice localizer and we asked an explorative research question of whether brain activation and/or classification accuracy maps in response to vocal (speech and non-speech) and non-vocal sounds differ between female and male listeners without prior assumptions about the strength of voice-specific activity.</p>
<p>The voice localizer paradigm is often used in the literature (Belin et al., <xref ref-type="bibr" rid="B5">2000</xref>, <xref ref-type="bibr" rid="B4">2002</xref>; Kreifelts et al., <xref ref-type="bibr" rid="B21">2009</xref>; Latinus et al., <xref ref-type="bibr" rid="B23">2011</xref>; Ethofer et al., <xref ref-type="bibr" rid="B10">2012</xref>), which makes it easier to compare among studies as well as among participants or groups. Instead of using the conventional univariate method, employing MVPA may offer a more sensitive approach in order to study potential differences between genders by means of above chance vocal/non-vocal classification accuracies in different regions of the brain. Therefore, we investigated our research question by implementing the conventional univariate analysis using GLM and MVPA based on a support-vector machine (SVM) classifier with a spherical searchlight approach. This approach enabled us to explore cortical activity over the whole-brain and to examine whether activation and/or classification maps in response to the voice localizer may significantly differ between genders. Since the effect size between genders is expected to be very small, the current study offers a substantially large sample size with <italic>n</italic> &#x0003D; 149 females and <italic>n</italic> &#x0003D; 123 males. Thus, this study provides a large sample size, a well-established experimental design and the direct comparison of two different fMRI data analysis approaches applied on the exact same data.</p>
</sec>
<sec sec-type="methods" id="s2">
<title>Methods</title>
<sec>
<title>Participants</title>
<p>fMRI data of 272 healthy participants, 149 female (age range: 18&#x02013;68 years; mean &#x000B1; <italic>SD</italic> &#x0003D; 24.5 &#x000B1; 8.0) and 123 male (age range: 18&#x02013;61 years; mean &#x000B1; <italic>SD</italic> &#x0003D; 24.4 &#x000B1; 6.5) with self-reported normal audition were analyzed. This study was conducted at the Institute of Neuroscience and Psychology (INP) in Glasgow and approved by the ethics committee of the University of Glasgow. Volunteers provided written informed consent before participating and were paid afterwards.</p>
</sec>
<sec>
<title>Voice localizer paradigm</title>
<p>Subjects were instructed to close their eyes and passively listen to a large variety of sounds. Stimuli were presented in a simple block design and divided into vocal (20 blocks) and non-vocal (20 blocks) conditions. Vocal blocks contained only sounds of human vocal origin (excluding sounds without vocal fold vibration such as whistling or whispering) and consisted of speech (e.g., words, syllables, connected speech in different languages) or non-speech (e.g., coughs, laughs, sighs and cries). The vocal stimuli consisted of recordings from 7 babies, 12 adults, 23 children, and 5 elderly people. Half of the vocal sounds (speech and non-speech) consisted of vocalizations from adults and elderly people (women and men) with comparable proportions for both genders (&#x0007E;24% female, &#x0007E;22% male). The other half of the vocal sounds consisted of infant vocalizations (speech and non-speech) which also included baby crying/laughing. Recorded non-vocal sounds included various environmental sounds (e.g., animal vocalizations, musical instruments, nature and industrial sounds). A total number of 40 blocks were presented. Each block lasted for 8 s with an inter-block interval of 2 s. Stimuli (16bit, mono, 22050 Hz sampling rate) were normalized for RMS and are available at <ext-link ext-link-type="uri" xlink:href="http://vnl.psy.gla.ac.uk/resources.php">http://vnl.psy.gla.ac.uk/resources.php</ext-link> (Belin et al., <xref ref-type="bibr" rid="B5">2000</xref>).</p>
</sec>
<sec>
<title>MRI data acquisition</title>
<p>Scanning was carried out in a 3T MR scanner (Magnetom Trio Siemens, Erlangen, Germany) and all data were acquired with the same scanner at the INP in Glasgow. Functional MRI volumes of the whole cortex were acquired using an echo-planar gradient pulse sequence (voxel size &#x0003D; 3 mm &#x000D7; 3 mm &#x000D7; 3 mm; Time of Repetition (TR) &#x0003D; 2000 ms; Echo Time (TE) &#x0003D; 30 ms; slice thickness &#x0003D; 3 mm; inter-slice gap &#x0003D; 0.3 mm; field of view (FoV) &#x0003D; 210 mm; matrix size &#x0003D; 70 &#x000D7; 70; excitation angle &#x0003D; 77&#x000B0;). A total number of 310 volumes (32 slices per volume, interleaved acquisition order) were collected with a total acquisition time of 10.28 min. Anatomical MRI volumes were acquired using a magnetization-prepared rapid gradient echo sequence (MPRAGE) (voxel size &#x0003D; 1 &#x000D7; 1 &#x000D7; 1 mm; <italic>TR</italic> &#x0003D; 1900 ms; <italic>TE</italic> &#x0003D; 2.52 ms; inversion time (TI) &#x0003D; 900 ms; slice thickness &#x0003D; 1 mm; FoV &#x0003D; 256 mm; matrix size &#x0003D; 256 &#x000D7; 265; excitation angle &#x0003D; 9&#x000B0;; 192 axial slices).</p>
</sec>
<sec>
<title>fMRI data analysis</title>
<sec>
<title>Pre-processing</title>
<p>Pre-processing was performed using the statistical parametric mapping software SPM8 (Department of Cognitive Neurology, London, UK. <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm/software/spm8/">http://www.fil.ion.ucl.ac.uk/spm/software/spm8/</ext-link>). After reorientation of functional and anatomical volumes to the AC/PC line (anterior- and posterior commissure), functional images were motion corrected (standard realignment). Since, subjects may have moved between anatomical and functional data acquisition, the anatomical volumes were co-registered to the mean functional image produced in the realignment above. Anatomical volumes were segmented in order to generate a binary gray matter template at threshold probability level of 0.5 for each individual participant. This template was applied during model specification in both univariate analysis und MVPA. For the univariate processing, realigned functional volumes were normalized to a standard MNI template (Montreal Neurological Institute) and spatially smoothed with a 6 mm full-width at half mean (FWHM) Gaussian Kernel.</p>
</sec>
<sec>
<title>Univariate analysis</title>
<p>The design matrix was defined such that each block of the experimental paradigm correlated to one condition, yielding a design matrix with 20 onsets for each condition (vocal and non-vocal). Analysis was based on the conventional general linear model (GLM) and stimuli were convolved with a boxcar hemodynamic response function provided by SPM8. Contrast images of vocal vs. non-vocal conditions were generated for each individual subject and entered into a second-level random effects analysis (RFX). To declare at the group-level whether any difference between the two conditions was significantly larger than zero, a one-sample <italic>t</italic>-test was applied and FWE-corrected (<italic>p</italic> &#x0003C; 0.05) brain maps were calculated. To investigate whether brain activity significantly differs between genders in response to vocal vs. non-vocal sounds, contrasts between females vs. males (male &#x0003E; female, female &#x0003E; male) were computed in a second level RFX analysis (two-sample <italic>t</italic>-test; <italic>p</italic> &#x0003C; 0.05 FWE-corrected). This analysis was restricted to voxels with classification accuracy significantly above theoretical chance (<italic>p</italic> &#x0003C; 0.01 uncorrected) in both females and males (see MVPA below and yellow area in <bold>Figure 2</bold>).</p>
</sec>
<sec>
<title>Multivariate pattern analysis</title>
<p>Multivariate pattern classification was performed on unsmoothed and non-normalized data using Matlab (Mathworks Inc., Natick, USA) and in-house utility scripts (INP, Voice Neurocognition Laboratory; Dr. Bashar Awwad Shiekh Hasan and Dr. Bruno L. Giordano), where the default linear support vector machine (SVM) classifier was applied. The classifier was trained and separately tested following a leave-one out cross validation strategy applied on the 40 beta parameter estimates obtained from the univariate analysis (GLM).</p>
<p>A whole-brain searchlight decoding analysis was implemented using a sphere with a radius of 6 mm (average number of voxels in one sphere: 20.6 &#x000B1; 1.0 <italic>SD</italic>) (Kriegeskorte et al., <xref ref-type="bibr" rid="B22">2006</xref>). A sphere was only considered for analysis if a minimum of 50% of its voxels were within the gray matter. The data of the voxels within a sphere were classified and the classification accuracy was stored at the central voxel, yielding a 3D brain map of classification accuracy (percentage of correct classifications) (Kriegeskorte et al., <xref ref-type="bibr" rid="B22">2006</xref>; Haynes et al., <xref ref-type="bibr" rid="B16">2007</xref>). To identify brain regions in which classification accuracy was significantly above chance by females and males, the theoretical chance level (50%) was subtracted, then normalized (to the MNI template) and smoothed (6 mm FWHM Gaussian Kernel). To make inference on female and male participants, classification brain maps were entered into a second-level permutation based analysis using statistical nonparametric mapping (SnPM; Statistical NonParametric Mapping; available at <ext-link ext-link-type="uri" xlink:href="http://warwick.ac.uk/snpm">http://warwick.ac.uk/snpm</ext-link>) with 10,000 permutations (see Holmes et al., <xref ref-type="bibr" rid="B17">1996</xref>; Nichols and Holmes, <xref ref-type="bibr" rid="B25">2002</xref>). This was computed separately by gender and the resulting voxels were assessed for significance at 5% level and FWE-corrected, as determined by permutation distribution. Similarly, to assess whether classification brain maps significantly differ between genders in response to vocal/non-vocal sounds, this permutation approach was implemented between groups (female &#x0003E; male, male &#x0003E; female) with 10,000 permutations and the resulting voxels were assessed for significance at 5% level and FWE-corrected, as determined by permutation distribution (see Holmes et al., <xref ref-type="bibr" rid="B17">1996</xref>; Nichols and Holmes, <xref ref-type="bibr" rid="B25">2002</xref>).</p>
<p>The between-group analysis was restricted to a mask defined by voxels with classification accuracy significantly above theoretical chance (<italic>p</italic> &#x0003C; 0.01 uncorrected) in both females and males. The resulting mask included 3783 voxels (yellow area in <bold>Figure 2</bold>). The same mask was applied for both, the univariate analysis and MVPA.</p>
<p>Separate brain maps of vocal vs. non-vocal contrast in female and male participants as well as brain maps of contrasts between genders for both, univariate analysis and MVPA were generated using the program MRIcoGL (available at <ext-link ext-link-type="uri" xlink:href="http://www.mccauslandcenter.sc.edu/mricro/mricron/">http://www.mccauslandcenter.sc.edu/mricro/mricron/</ext-link>).</p>
</sec>
</sec>
</sec>
<sec sec-type="results" id="s3">
<title>Results</title>
<sec>
<title>Univariate analysis: vocal vs. non-vocal sounds</title>
<p>The univariate analysis comparing activation to vocal and non-vocal sounds showed extended areas of greater response to vocal sounds in the typical regions of the temporal voice areas (TVA), highly similar for male and female subjects (Figure <xref ref-type="fig" rid="F1">1A</xref>). These regions were located bilaterally in the temporal lobes extending from posterior parts of the STS along the STG to anterior parts of the STS and also including several parts of the superior and middle temporal gyrus (STG, MTG).</p>
<fig id="F1" position="float">
<label>Figure 1</label>
<caption><p><bold>Brain maps of female (red, <italic>n</italic> &#x0003D; 149) and male (blue, <italic>n</italic> &#x0003D; 123) participants</bold>. <bold>(A)</bold> Univariate analysis showing bilateral activation along the superior temporal sulcus (STS) and in the inferior frontal gyrus (IFG) and corresponding contrast estimates of vocal vs. non-vocal sounds plotted for peak voxel (one-sample <italic>t</italic>-test, FWE-corrected, <italic>p</italic> &#x0003C; 0.05; cf. circles, note that the two peaks with highest <italic>T</italic>-value and largest cluster size are indicated per group). <bold>(B)</bold> MVPA showing comparable classification accuracy maps along STS, but not IFG and average classification accuracy &#x000B1; s.e.m. at peak voxel (calculated in native space) was distinctly above chance level (0.5) for both females and males (maximum intensity projection of <italic>t</italic>-statistic image threshold at FWE-corrected <italic>p</italic> &#x0003C; 0.05, as determined by permutation distribution with 10,000 permutations).</p></caption>
<graphic xlink:href="fnins-08-00228-g0001.tif"/>
</fig>
<p>Several hemispheric maxima of vocal vs. non-vocal response were located bilaterally along the STS in both females and males (Figure <xref ref-type="fig" rid="F1">1</xref>, Table <xref ref-type="table" rid="T1">1</xref>). Figure <xref ref-type="fig" rid="F1">1A</xref> shows parameter estimates of the vocal &#x0003E; non-vocal contrasts at the maxima of the largest cluster sizes with the highest <italic>T</italic>-values of each hemisphere. The brain activation differences between vocal and non-vocal response was consistent across maxima in females (MNI coordinates left: <italic>x</italic> &#x0003D; &#x02212;57, <italic>y</italic> &#x0003D; &#x02212;16, <italic>z</italic> &#x0003D; &#x02212;2, cluster size 3923, <italic>T</italic> &#x0003D; 20.85; right: <italic>x</italic> &#x0003D; 60, <italic>y</italic> &#x0003D; &#x02212;13, <italic>z</italic> &#x0003D; &#x02212;2, <italic>T</italic>-value &#x0003D; 20.64) and in males (MNI coordinates left: <italic>x</italic> &#x0003D; &#x02212;60, <italic>y</italic> &#x0003D; &#x02212;22, <italic>z</italic> &#x0003D; 1, cluster size 796, <italic>T</italic> &#x0003D; 18.19; right: <italic>x</italic> &#x0003D; 60, <italic>y</italic> &#x0003D; &#x02212;10, <italic>z</italic> &#x0003D; &#x02212;2, cluster size 812, <italic>T</italic>-value &#x0003D; 17.46). Female listeners showed one large cluster covering the temporal lobes and subcortical parts of the brain. By contrast male listeners showed two separate voxel clusters in the left and right temporal lobes and no subcortical cluster connecting the two hemispheres (Table <xref ref-type="table" rid="T1">1</xref>). Small bilateral clusters were found in inferior prefrontal cortex (inferior frontal gyrus, IFG) in both female and male listeners (<italic>p</italic> &#x0003C; 0.05 FWE-corrected; Figure <xref ref-type="fig" rid="F1">1A</xref>).</p>
<table-wrap position="float" id="T1">
<label>Table 1</label>
<caption><p><bold>Voice-sensitive peak voxels of female and male RFX analysis (Univariate)</bold>.</p></caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left"><bold>Anatomical location</bold></th>
<th align="center"><bold>Peak voxel <italic>x</italic>, <italic>y</italic>, <italic>z</italic></bold></th>
<th align="center"><bold><italic>t</italic>-values</bold></th>
<th align="center"><bold>Cluster size</bold></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="4"><bold>FEMALE LISTENERS</bold></td>
</tr>
<tr>
<td align="left" colspan="4"><bold>Left/Right hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>Left STG, middle</bold></td>
<td align="center">&#x02212;57, &#x02212;16, &#x02212;2</td>
<td align="center">20.85</td>
<td align="center">3923</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;Right STG, middle</td>
<td align="center">60, &#x02212;13, &#x02212;2</td>
<td align="center">20.64</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;Right STG, middle</td>
<td align="center">63, &#x02212;22, &#x02212;2</td>
<td align="center">20.11</td>
<td/>
</tr>
<tr>
<td align="left" colspan="4"><bold>Left frontal hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>IFG (pars triangularis)</bold></td>
<td align="center">&#x02212;48, 17, 22</td>
<td align="center">9.25</td>
<td align="center">178</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;IFG (pars triangularis)</td>
<td align="center">&#x02212;39, 29, &#x02212;2</td>
<td align="center">8.79</td>
<td align="center">103</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;Precentral gyrus</td>
<td align="center">&#x02212;48, &#x02212;7, 43</td>
<td align="center">6.32</td>
<td align="center">5</td>
</tr>
<tr>
<td align="left" colspan="4"><bold>Right frontal hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>IFG (orbital)</bold></td>
<td align="center">48, 17, &#x02212;8</td>
<td align="center">4.87</td>
<td align="center">1</td>
</tr>
<tr>
<td align="left" colspan="4"><bold>MALE LISTENERS</bold></td>
</tr>
<tr>
<td align="left" colspan="4"><bold>Left hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>STG, middle</bold></td>
<td align="center">&#x02212;60, &#x02212;22, 1</td>
<td align="center">18.15</td>
<td align="center">796</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, middle</td>
<td align="center">&#x02212;57, &#x02212;13, &#x02212;2</td>
<td align="center">17.97</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, posterior</td>
<td align="center">&#x02212;60, &#x02212;37, 4</td>
<td align="center">12.73</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>IFG (pars triangularis)</bold></td>
<td align="center">&#x02212;42, 29, &#x02212;2</td>
<td align="center">7.96</td>
<td align="center">40</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>IFG (pars triangularis)</bold></td>
<td align="center">&#x02212;42, 17, 22</td>
<td align="center">5.56</td>
<td align="center">32</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>Hippocampus</bold></td>
<td align="center">&#x02212;18, &#x02212;10, &#x02212;14</td>
<td align="center">4.61</td>
<td align="center">1</td>
</tr>
<tr>
<td align="left" colspan="4"><bold>Right hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>STG, middle</bold></td>
<td align="center">60, &#x02212;10, &#x02212;2</td>
<td align="center">17.40</td>
<td align="center">812</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, middle</td>
<td align="center">63, &#x02212;22, &#x02212;2</td>
<td align="center">17.11</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, anterior</td>
<td align="center">54, 5, &#x02212;14</td>
<td align="center">11.51</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>IFG (pars triangularis)</bold></td>
<td align="center">42, 32, &#x02212;2</td>
<td align="center">6.76</td>
<td align="center">165</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;IFG (pars triangularis)</td>
<td align="center">54, 23, 22</td>
<td align="center">6.62</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;IFG (pars triangularis)</td>
<td align="center">45, 17, 22</td>
<td align="center">6.51</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>Precentral gyrus</bold></td>
<td align="center">51, &#x02212;1, 46</td>
<td align="center">7.60</td>
<td align="center">22</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<p><italic>Peak voxel coordinates in standard MNI space and corresponding t-values above for female and male 4.49 (FWE-corrected p &#x0003C; 0.05).</italic></p>
</table-wrap-foot>
</table-wrap>
</sec>
<sec>
<title>MVPA analysis: vocal/non-vocal classification</title>
<p>The MVPA analysis showed clusters of significantly above-chance voice/non-voice classification accuracy in the TVAs (Figure <xref ref-type="fig" rid="F1">1B</xref>, Table <xref ref-type="table" rid="T1">1</xref>) (Figure <xref ref-type="fig" rid="F1">1A</xref>, Table <xref ref-type="table" rid="T2">2</xref>). Hemispheric maxima of classification accuracy were at comparable locations as the peaks of voice &#x0003E; non-voice activation revealed by the univariate method. The classification accuracy within the peak voxel of female listeners (MNI coordinates left: <italic>x</italic> &#x0003D; &#x02212;60, <italic>y</italic> &#x0003D; &#x02212;16, <italic>z</italic> &#x0003D; 1, cluster size 1676, <italic>T</italic>-value &#x0003D; 20.41; right: <italic>x</italic> &#x0003D; 66, <italic>y</italic> &#x0003D; &#x02212;31, <italic>z</italic> &#x0003D; 4, cluster size 1671, <italic>T</italic>-value &#x0003D; 21.45) as well as for male listeners (MNI coordinates left: <italic>x</italic> &#x0003D; &#x02212;60, <italic>y</italic> &#x0003D; &#x02212;22, <italic>z</italic> &#x0003D; 4, cluster size 984, <italic>T</italic>-value &#x0003D; 13.70; right: <italic>x</italic> &#x0003D; 63, <italic>y</italic> &#x0003D; &#x02212;28, <italic>z</italic> &#x0003D; 4, cluster size 1211, <italic>T</italic>-value &#x0003D; 16.07) were distincly above the theoretical chance level of 0.5 (Figure <xref ref-type="fig" rid="F1">1B</xref>). Overall, the maximal classification accuracy was higher in female listeners as compared to male listeners at the peak voxels (Figure <xref ref-type="fig" rid="F1">1B</xref>, mean &#x000B1; s.e.m.: left peak in females 0.84 &#x000B1; 0.006, males 0.83 &#x000B1; 0.009; right peak in females 0.85 &#x000B1; 0.007, males 0.84 &#x000B1; 0.009. Left peak in males 0.83 &#x000B1; 0.009, females 0.85 &#x000B1; 0.006, right peak in males 0.85 &#x000B1; 0.009, females 0.87 &#x000B1; 0.007). Comparing MVPA and univariate analysis in Figures <xref ref-type="fig" rid="F1">1A,B</xref> the MVPA analysis revealed more superficial cortical regions bilateral at the temporal pole, whereas the voxel cluster of the vocal vs. non-vocal difference of the univariate analysis extend more toward the midline of the brain.</p>
<table-wrap position="float" id="T2">
<label>Table 2</label>
<caption><p><bold>Voice-sensitive peak voxels of female and male group analysis (MVPA)</bold>.</p></caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left"><bold>Anatomical location</bold></th>
<th align="center"><bold>Peak voxel <italic>x</italic>, <italic>y</italic>, <italic>z</italic></bold></th>
<th align="center"><bold><italic>t</italic>-values</bold></th>
<th align="center"><bold>Cluster size</bold></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="4"><bold>FEMALE LISTENERS</bold></td>
</tr>
<tr>
<td align="left" colspan="4"><bold>Left hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>MTG, anterior</bold></td>
<td align="center">&#x02212;60, &#x02212;16, 1</td>
<td align="center">20.41</td>
<td align="center">1676</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;MTG, posterior</td>
<td align="center">&#x02212;63, &#x02212;37, 7</td>
<td align="center">18.26</td>
<td/>
</tr>
<tr>
<td align="left" colspan="4"><bold>Right hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>STG, middle</bold></td>
<td align="center">66, &#x02212;31, 4</td>
<td align="center">21.45</td>
<td align="center">1671</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, anterior</td>
<td align="center">60, &#x02212;7, &#x02212;5</td>
<td align="center">19.49</td>
<td/>
</tr>
<tr>
<td align="left" colspan="4"><bold>MALE LISTENERS</bold></td>
</tr>
<tr>
<td align="left" colspan="4"><bold>Left hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>MTG, middle</bold></td>
<td align="center">&#x02212;60, &#x02212;22, 4</td>
<td align="center">13.70</td>
<td align="center">984</td>
</tr>
<tr>
<td align="left" colspan="4"><bold>Right hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>STG, middle</bold></td>
<td align="center">63, &#x02212;28, 4</td>
<td align="center">16.07</td>
<td align="center">1211</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;MTG, anterior</td>
<td align="center">63, &#x02212;10, &#x02212;5</td>
<td align="center">14.88</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<p><italic>Peak voxel coordinates in standard MNI space and corresponding t-values above for female 4.38 and male 4.29 (FWE-corrected p &#x0003C; 0.05, as determined by permutation distribution with 10,000 permutations).</italic></p>
</table-wrap-foot>
</table-wrap>
</sec>
<sec>
<title>Female vs. male contrasts</title>
<p>The contrast of activation maps (univariate analysis) or classification accuracy maps (multivariate approach) from males and females revealed no significant voxels with greater parameter estimates for males &#x0003E; females at the chosen statistical significance threshold (<italic>p</italic> &#x0003C; 0.05, FWE-corrected) for either analysis methods. The reverse contrast (female &#x0003E; male), however, revealed significant voxel clusters showing greater parameter estimates for univariate analysis and higher classification accuracy for MVPA in female participants (Figure <xref ref-type="fig" rid="F2">2</xref>).</p>
<fig id="F2" position="float">
<label>Figure 2</label>
<caption><p><bold>Contrast between female &#x0003E; male (red)</bold>. <bold>(A)</bold> Univariate analysis showing significant female &#x0003E; male difference (two-sample <italic>t</italic>-test, FWE-corrected, <italic>p</italic> &#x0003C; 0.05) in the left posterior part of the superior temporal gyrus (STG) and the right anterior STG. Contrast estimates at peak voxel showing stronger activation in females (black) as compared to males (gray) in response to vocal vs. non-vocal sounds. <bold>(B)</bold> MVPA showing significant classification accuracy above chance level in the right middle part of the middle temporal gyrus (MTG) and the right middle STG as well as in the left middle MTG with higher average classification accuracy in females (black) than in males (gray) (maximum intensity projection of <italic>t</italic>-statistic image threshold at FWE-corrected <italic>p</italic> &#x0003C; 0.05, as determined by permutation distribution with 10,000 permutations). The (yellow) cluster shows the mask including voxels with significantly above chance classification accuracy in both females and males (<italic>p</italic> &#x0003C; 0.01 uncorrected).</p></caption>
<graphic xlink:href="fnins-08-00228-g0002.tif"/>
</fig>
<p>When analyzed with the univariate approach (Figure <xref ref-type="fig" rid="F2">2A</xref>) the contrast female &#x0003E; male yielded only a few significant voxels: One cluster consisted of four voxels in the left posterior part of STG and only one voxels in the right Insula (Figure <xref ref-type="fig" rid="F2">2A</xref>, Table <xref ref-type="table" rid="T3">3</xref>). The corresponding contrast estimates for the reported peak voxels (MNI coordinates left: <italic>x</italic> &#x0003D; &#x02212;48, <italic>y</italic> &#x0003D; &#x02212;34, <italic>z</italic> &#x0003D; 16, cluster size 4, <italic>T</italic>-value &#x0003D; 4.02; right: <italic>x</italic> &#x0003D; 48, <italic>y</italic> &#x0003D; 2, <italic>z</italic> &#x0003D; &#x02212;5, cluster size &#x0003D; 1, <italic>T</italic>-value &#x0003D; 4.04) showed a positive response for females in both hemispheres and for the left hemisphere in males. The Cohen&#x00027;s d effect size values (<italic>d</italic> &#x0003D; 0.48 and 0.49) suggested a moderate difference at the peak voxel (Table <xref ref-type="table" rid="T3">3</xref>). Overall, females showed a stronger activation in response to vocal vs. non-vocal sounds as compared to males at both maxima (Figure <xref ref-type="fig" rid="F2">2A</xref>).</p>
<table-wrap position="float" id="T3">
<label>Table 3</label>
<caption><p><bold>Peak voxels of female &#x0003E; male contrast for univariate analysis and MVPA</bold>.</p></caption>
<table frame="hsides" rules="groups">
<thead>
<tr>
<th align="left"><bold>Anatomical location</bold></th>
<th align="center"><bold>Peak voxel <italic>x</italic>, <italic>y</italic>, <italic>z</italic></bold></th>
<th align="center"><bold><italic>t</italic>-values</bold></th>
<th align="center"><bold>Cluster size</bold></th>
<th align="center"><bold>Cohen&#x00027;s d at the peak voxel</bold></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" colspan="5"><bold>UNIVARIATE (FEMALE &#x0003E; MALE)</bold></td>
</tr>
<tr>
<td align="left" colspan="5"><bold>Left hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>STG, posterior</bold></td>
<td align="center">&#x02212;48, &#x02212;34, 16</td>
<td align="center">4.02</td>
<td align="center">4</td>
<td align="center">0.48</td>
</tr>
<tr>
<td align="left" colspan="5"><bold>Right hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>Insula</bold></td>
<td align="center">48, 2, &#x02212;5</td>
<td align="center">4.04</td>
<td align="center">1</td>
<td align="center">0.49</td>
</tr>
<tr>
<td align="left" colspan="5"><bold>MVPA (FEMALE &#x0003E; MALE)</bold></td>
</tr>
<tr>
<td align="left" colspan="5"><bold>Left hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>STG, middle</bold></td>
<td align="center">&#x02212;69, &#x02212;19, &#x02212;8</td>
<td align="center">5.22</td>
<td align="center">84</td>
<td align="center">0.35</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, middle</td>
<td align="center">&#x02212;66, &#x02212;1, &#x02212;8</td>
<td align="center">5.02</td>
<td/>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>STG, middle</bold></td>
<td align="center">&#x02212;51, &#x02212;22, 13</td>
<td align="center">5.19</td>
<td align="center">156</td>
<td align="center">0.35</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, middle</td>
<td align="center">&#x02212;48, &#x02212;31, 4</td>
<td align="center">4.77</td>
<td/>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, posterior</td>
<td align="center">&#x02212;42, &#x02212;43, 7</td>
<td align="center">4.66</td>
<td/>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;MTG, middle</td>
<td align="center">&#x02212;57, &#x02212;55, 16</td>
<td align="center">3.82</td>
<td align="center">2</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;MTG, middle</td>
<td align="center">&#x02212;69, &#x02212;40, 1</td>
<td align="center">3.80</td>
<td align="center">2</td>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;STG, middle</td>
<td align="center">&#x02212;69, &#x02212;10, 10</td>
<td align="center">3.79</td>
<td align="center">2</td>
<td/>
</tr>
<tr>
<td align="left" colspan="5"><bold>Right hemisphere</bold></td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;<bold>STG, middle</bold></td>
<td align="center">69, &#x02212;7, &#x02212;11</td>
<td align="center">4.48</td>
<td align="center">52</td>
<td align="center">0.24</td>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;MTG, middle</td>
<td align="center">66, &#x02212;22, &#x02212;11</td>
<td align="center">4.42</td>
<td/>
<td/>
</tr>
<tr>
<td align="left">&#x000A0;&#x000A0;&#x000A0;MTG, middle</td>
<td align="center">69, &#x02212;34, 1</td>
<td align="center">3.70</td>
<td align="center">1</td>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<p><italic>Peak voxel coordinates in standard MNI space and corresponding t-values above 3.85 (univariate analysis, FWE-corrected p &#x0003C; 0.05) and 3.70 for MVPA (FWE-corrected p &#x0003C; 0.05, as determined by permutation distribution with 10,000 permutations) and Cohen&#x00027;s d for large cluster size. The Cohen&#x00027;s d of the MVPA refers to the mean difference in classification accuracy (contrast estimates of the univariate analysis respectively), divided by the pooled standard deviation for those means.</italic></p>
</table-wrap-foot>
</table-wrap>
<p>The female &#x0003E; male contrast of classification accuracy maps identified significant voxel clusters in the middle part of the middle temporal gyrus (MTG) in both hemispheres, in which classification accuracy was greater for female than male participants (red clusters in Figure <xref ref-type="fig" rid="F2">2B</xref>). Areas of greater classification accuracy in females were more extended in the left hemisphere with an additional smaller cluster located in the STG. The peak voxels of female &#x0003E; male classification accuracy difference were located in the middle part of the MTG (bilateral), and the left middle STG (MNI coordinates left: <italic>x</italic> &#x0003D; &#x02212;69, <italic>y</italic> &#x0003D; &#x02212; 19, <italic>z</italic> &#x0003D; &#x02212;8, cluster size 84, <italic>T</italic>-value &#x0003D; 5.22; <italic>x</italic> &#x0003D; &#x02212;51, <italic>y</italic> &#x0003D; &#x02212;22, <italic>z</italic> &#x0003D; 13, cluster size 156, <italic>T</italic>-value &#x0003D; 5.19; right: <italic>x</italic> &#x0003D; 69, <italic>y</italic> &#x0003D; &#x02212;7, <italic>z</italic> &#x0003D; &#x02212;11, cluster size 52, <italic>T</italic>-value &#x0003D; 4.48; cf. circle in Figure <xref ref-type="fig" rid="F2">2A</xref>). The Cohen&#x00027;s d effect size values (<italic>d</italic> &#x0003D; 0.35, 0.35, and 0.24) suggested a small difference at the peak voxel (Table <xref ref-type="table" rid="T3">3</xref>). Classification accuracy (computed in native space) at these coordinates was distinctly above chance (50%) for both females and males, but higher in females across peaks (Figure <xref ref-type="fig" rid="F2">2B</xref>).</p>
</sec>
</sec>
<sec sec-type="discussion" id="s4">
<title>Discussion</title>
<p>The present study aimed to investigate gender differences on voice localizer scans by employing the conventional univariate analysis as well as MVPA. Both analysis approaches revealed largely overlapping/comparable and robust estimates of the TVAs in female and male listeners. However, the MVPA was more sensitive to differences in the middle MTG of the left and right hemispheres and the middle left STG between genders as compared to univariate analysis with higher classification accuracy in women.</p>
</sec>
<sec>
<title>Robust TVAs</title>
<p>The estimated TVAs using MVPA robustly replicated and confirmed prior fMRI findings applying the voice localizer (Belin et al., <xref ref-type="bibr" rid="B5">2000</xref>, <xref ref-type="bibr" rid="B4">2002</xref>; Belin and Zatorre, <xref ref-type="bibr" rid="B3">2003</xref>; Scott and Johnsrude, <xref ref-type="bibr" rid="B33">2003</xref>; Von Kriegstein et al., <xref ref-type="bibr" rid="B37">2003</xref>). Both analysis methods showed comparable maps of classification accuracy (MVPA) and of vocal vs. non-vocal activity difference (univariate analysis) for both female and male listeners. The average classification accuracy at the peak voxel was distinctly above chance level and higher in female as compared to male listeners. The peak voxels were at comparable locations (along middle and posterior parts of the STS) for both analysis approaches and both genders. A small difference between the MVPA and univariate analysis can be seen bilateral at the temporal pole, where the MVPA detected more vocal/non-vocal differences in superficial cortical regions as compared to the univariate analysis. In addition to the activation brain maps showing the robustly estimated TVAs (univariate analysis), the MVPA results extend previous findings by providing a corresponding classification accuracy brain map. When brain maps are considered for each analysis approach and for female and male listeners separately, our findings showed no distinct differences between genders and between univariate analysis and MVPA. Instead comparable voxel clusters of a similar size in the bilateral temporal lobes were identified, verifying the prior univariate analysis and the robustness of the TVAs (see e.g., Belin et al., <xref ref-type="bibr" rid="B5">2000</xref>).</p>
</sec>
<sec>
<title>Gender differences</title>
<p>When data were analyzed with MVPA, differences between female and male listeners in response to vocal/non-vocal sounds were found by contrasting female &#x0003E; male (but not male &#x0003E; female). A significant difference in success of the MVPA between female and male listeners was apparent in the middle part of the MTG in both hemispheres and in the middle part of the STG in the left hemisphere. Effect sizes showed a small difference at the peak voxels. Despite the large sample size used in this study, the univariate analysis showed no major activation differences between genders. Only two small clusters with one to four voxels were significant in the posterior and anterior part of the STG. In the univariate analysis, the overall activation difference between vocal vs. non-vocal sounds was stronger in female as compared to male listeners and effect sizes showed a moderate difference at the peak voxels.</p>
<p>The distinct gender differences located in the middle part of MTG and middle part of STG between genders revealed by the MVPA survived our applied criteria (FWE-correction). In these regions, the classifier successfully distinguished between the vocal and non-vocal condition with better overall accuracy in females as compared to males across the peak voxels. Thus, BOLD signal in parts of auditory cortex seem to carry less information for discriminating vocal from nonvocal sounds in male than females listeners. We do not make any inference on the nature of the underlying processing differences in terms of mental states or cognitive mechanisms, but possible explanations for our findings are discussed below.</p>
<p>MVPA may overall be more sensitive to detect small differences in the activation patterns to vocal and non-vocal sounds. Thus, differences between genders appear significant only when analyzed with MVPA (Haynes et al., <xref ref-type="bibr" rid="B16">2007</xref>; Kriegeskorte et al., <xref ref-type="bibr" rid="B22">2006</xref>; Norman et al., <xref ref-type="bibr" rid="B26">2006</xref>). The differences in classification accuracy between female and male listeners, identified in parts of auditory cortex, may be contributed to by a different predisposition of female/male listeners to the presented vocal sound samples of the voice localizer. Previous findings suggest a sex-difference in response to infant crying and laughing. Women showed a deactivation in the anterior cingulate cortex (ACC) to both laughing and crying (independent of parental status) as compared to men (Seifritz et al., <xref ref-type="bibr" rid="B34">2003</xref>). In contrast, another study showed increased activation to infant vocalization in the amygdala and ACC whereas men showed increased activation to the control stimuli (fragment recombined and edge smoothed stimuli of the original laughing/crying samples). This may reflect a tendency in women for a response preference to infant vocal expressions (Sander et al., <xref ref-type="bibr" rid="B27">2007</xref>). A recent study by De Pisapia et al. (<xref ref-type="bibr" rid="B9">2013</xref>) found a sex-difference in response to a baby cry. Women decreased brain activity in DPFC regions and posterior cingulate cortex when they suddenly and passively heard infant cries, whereas men did not. They interpreted their findings in such a way that the female brain interrupts on-going mind-wandering during cries and the male brain continues in self-reflection (De Pisapia et al., <xref ref-type="bibr" rid="B9">2013</xref>). In our study half of the vocal stimuli consisted of infant vocalizations (also emotional expressions such as laughing and crying) and our results may reflect differences in the fine-grained pattern of distributed activity in female and male listeners in response to these vocal expressions of children and babies. The outcome in this study may be affected by anatomical differences in brain structure/size between female and male listeners (Brett et al., <xref ref-type="bibr" rid="B7">2002</xref>). In general individuals vary in their anatomical brain structures and undergo the experiment with different mental states which may influence their brain responses (Huettel et al., <xref ref-type="bibr" rid="B18">2008</xref>).</p>
<p>To date, there is also evidence for differences in the vocal processing and in particular in speech perception between genders from both behavioral (Hall, <xref ref-type="bibr" rid="B12">1978</xref>; Skuk and Schweinberger, <xref ref-type="bibr" rid="B36">2013</xref>) and previous fMRI studies (Shaywitz et al., <xref ref-type="bibr" rid="B35">1995</xref>; Schirmer et al., <xref ref-type="bibr" rid="B29">2002</xref>, <xref ref-type="bibr" rid="B32">2004</xref>, <xref ref-type="bibr" rid="B30">2007</xref>; Junger et al., <xref ref-type="bibr" rid="B19">2013</xref>). These studies found activation differences in frontal brain regions (Schirmer et al., <xref ref-type="bibr" rid="B32">2004</xref>; Junger et al., <xref ref-type="bibr" rid="B19">2013</xref>) and the left posterior MTG and the angular gyrus (Junger et al., <xref ref-type="bibr" rid="B19">2013</xref>). The deviation of the current results in terms of identified brain regions may be due to the different experimental design and computed contrasts, the different applied criteria (e.g., mask), number of included participants and implemented analysis methods. Future studies should further aim to elucidate the relationships between behavioral and functional activation differences. However, the current study shows that the choice of fMRI analysis method (e.g., MVPA) is of relevance when considering subtle between-gender differences.</p>
<p>Regarding the current study, it would be interesting to separate the different vocal categories in the analysis (e.g., by speaker: female/male adults vs. infants/babies) and to perform a behavioral task in order to link differences in brain activation to behavior of the listener. Furthermore, it would be interesting for future studies to take into account more specific aspects of voice quality, which were not considered in the current study. Even subtle differences in phonation (e.g., whispery voice, harshness of a voice), articulation (e.g., vowel space) and or prosody (e.g., pitch variability, loudness, tempo) are critical aspects of voice processing and could be investigated using similar methodical approaches. Apart from studying differences between women and men, also other listener characteristics, such as differences between young and elderly participants, different nationalities and/or familiarity with the presented voices/stimuli should be considered.</p>
</sec>
<sec sec-type="conclusion" id="s5">
<title>Conclusion</title>
<p>Male and female participants were similar in their pattern of activity differences in response to vocal vs. nonvocal sounds in the TVA of the auditory cortex. Yet, MVPA revealed several regions of significant gender differences in classification performance between female and male listeners: in these regions the distributed pattern of local activity from female participants allowed significantly better vocal/nonvocal classification than that of male participants; no region showed the opposite male &#x0003E; female difference. The neuronal mechanims underlying the observed differences remain unclear.</p>
<sec>
<title>Conflict of interest statement</title>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec>
</sec>
</body>
<back>
<ack>
<p>We thank David Fleming for helpful matlab support and Chris Benwell for English corrections.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="B1">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname> <given-names>P.</given-names></name> <name><surname>Bestelmeyer</surname> <given-names>P. E. G.</given-names></name> <name><surname>Latinus</surname> <given-names>M.</given-names></name> <name><surname>Watson</surname> <given-names>R.</given-names></name></person-group> (<year>2011</year>). <article-title>Understanding voice perception</article-title>. <source>Br. J. Psychol</source>. <volume>102</volume>, <fpage>711</fpage>&#x02013;<lpage>725</lpage>. <pub-id pub-id-type="doi">10.1111/j.2044-8295.2011.02041.x</pub-id><pub-id pub-id-type="pmid">21988380</pub-id></citation>
</ref>
<ref id="B2">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname> <given-names>P.</given-names></name> <name><surname>Shirley</surname> <given-names>F.</given-names></name> <name><surname>Catherine</surname> <given-names>B.</given-names></name></person-group> (<year>2004</year>). <article-title>Thinking the voice: neural correlates of voice perception</article-title>. <source>Trends Cogn. Sci</source>. <volume>8</volume>, <fpage>129</fpage>&#x02013;<lpage>135</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2004.01.008</pub-id><pub-id pub-id-type="pmid">15301753</pub-id></citation>
</ref>
<ref id="B3">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname> <given-names>P.</given-names></name> <name><surname>Zatorre</surname> <given-names>R. J.</given-names></name></person-group> (<year>2003</year>). <article-title>Adaptation to speaker&#x00027;s voice in right anterior temporal lobe</article-title>. <source>Neuroreport</source> <volume>14</volume>, <fpage>2105</fpage>&#x02013;<lpage>2109</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200311140-00019</pub-id><pub-id pub-id-type="pmid">14600506</pub-id></citation>
</ref>
<ref id="B4">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname> <given-names>P.</given-names></name> <name><surname>Zatorre</surname> <given-names>R. J.</given-names></name> <name><surname>Ahad</surname> <given-names>P.</given-names></name></person-group> (<year>2002</year>). <article-title>Human temporal-lobe response to vocal sounds</article-title>. <source>Cogn. Brain Res</source>. <volume>13</volume>, <fpage>17</fpage>&#x02013;<lpage>26</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(01)00084-2</pub-id><pub-id pub-id-type="pmid">11867247</pub-id></citation>
</ref>
<ref id="B5">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname> <given-names>P.</given-names></name> <name><surname>Zatorre</surname> <given-names>R. J.</given-names></name> <name><surname>Lafaille</surname> <given-names>P.</given-names></name> <name><surname>Ahad</surname> <given-names>P.</given-names></name> <name><surname>Pike</surname> <given-names>B.</given-names></name></person-group> (<year>2000</year>). <article-title>Voice-selective areas in human auditory cortex</article-title>. <source>Nature</source> <volume>403</volume>, <fpage>309</fpage>&#x02013;<lpage>312</lpage>. <pub-id pub-id-type="doi">10.1038/35002078</pub-id><pub-id pub-id-type="pmid">10659849</pub-id></citation>
</ref>
<ref id="B6">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Binder</surname> <given-names>J. R.</given-names></name> <name><surname>Frost</surname> <given-names>J. A.</given-names></name> <name><surname>Hammeke</surname> <given-names>T. A.</given-names></name> <name><surname>Bellgowan</surname> <given-names>P. S.</given-names></name> <name><surname>Springer</surname> <given-names>J. A.</given-names></name> <name><surname>Kaufman</surname> <given-names>J. N.</given-names></name> <etal/></person-group>. (<year>2000</year>). <article-title>Human temporal lobe activation by speech and nonspeech sounds</article-title>. <source>Cereb. Cortex</source> <volume>10</volume>, <fpage>512</fpage>&#x02013;<lpage>528</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/10.5.512</pub-id><pub-id pub-id-type="pmid">10847601</pub-id></citation>
</ref>
<ref id="B7">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Brett</surname> <given-names>M.</given-names></name> <name><surname>Johnsrude</surname> <given-names>I. S.</given-names></name> <name><surname>Owen</surname> <given-names>A. M.</given-names></name></person-group> (<year>2002</year>). <article-title>The problem of functional localization in the human brain</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>3</volume>, <fpage>243</fpage>&#x02013;<lpage>249</lpage>. <pub-id pub-id-type="doi">10.1038/nrn756</pub-id><pub-id pub-id-type="pmid">11994756</pub-id></citation>
</ref>
<ref id="B8">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname> <given-names>D. D.</given-names></name> <name><surname>Savoy</surname> <given-names>R. L.</given-names></name></person-group> (<year>2003</year>). <article-title>Functional magnetic resonance imaging (fMRI) &#x0201C;brain reading&#x0201D;: detecting and classifying distributed patterns of fMRI activity in human visual cortex</article-title>. <source>Neuroimage</source>, <volume>19</volume>, <fpage>261</fpage>&#x02013;<lpage>270</lpage>. <pub-id pub-id-type="doi">10.1016/S1053-8119(03)00049-1</pub-id><pub-id pub-id-type="pmid">12814577</pub-id></citation>
</ref>
<ref id="B9">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>De Pisapia</surname> <given-names>N.</given-names></name> <name><surname>Bornstein</surname> <given-names>M. H.</given-names></name> <name><surname>Rigo</surname> <given-names>P.</given-names></name> <name><surname>Esposito</surname> <given-names>G.</given-names></name> <name><surname>De Falco</surname> <given-names>S.</given-names></name> <name><surname>Venuti</surname> <given-names>P.</given-names></name></person-group> (<year>2013</year>). <article-title>Sex differences in directional brain responses to infant hunger cries</article-title>. <source>Neuroreport</source> <volume>24</volume>, <fpage>142</fpage>&#x02013;<lpage>146</lpage>. <pub-id pub-id-type="doi">10.1097/WNR.0b013e32835df4fa</pub-id><pub-id pub-id-type="pmid">23282991</pub-id></citation>
</ref>
<ref id="B10">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname> <given-names>T.</given-names></name> <name><surname>Bretscher</surname> <given-names>J.</given-names></name> <name><surname>Geschwind</surname> <given-names>M.</given-names></name> <name><surname>Bejamin</surname> <given-names>K.</given-names></name> <name><surname>Wildgruber</surname> <given-names>D.</given-names></name> <name><surname>Vuilleumier</surname> <given-names>P.</given-names></name></person-group> (<year>2012</year>). <article-title>Emotional voice areas: anatomic location, functional properties, and structural connections revealed by combined fMRI/DTI</article-title>. <source>Cereb. Cortex</source> <volume>22</volume>, <fpage>191</fpage>&#x02013;<lpage>200</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhr113</pub-id><pub-id pub-id-type="pmid">21625012</pub-id></citation>
</ref>
<ref id="B11">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname> <given-names>G. B. C.</given-names></name> <name><surname>Witelson</surname> <given-names>C. A. S. F.</given-names></name> <name><surname>Szechtman</surname> <given-names>H.</given-names></name> <name><surname>Nahmias</surname> <given-names>C.</given-names></name></person-group> (<year>2004</year>). <article-title>Sex difference in functional activation patterns revealed by increased emotion processing demands</article-title>. <source>Neuroreport</source> <volume>15</volume>, <fpage>219</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1097/01.wnr.0000101310.64109.94</pub-id><pub-id pub-id-type="pmid">15076740</pub-id></citation>
</ref>
<ref id="B12">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname> <given-names>J. A.</given-names></name></person-group> (<year>1978</year>). <article-title>Gender effects in decoding nonverbal cues</article-title>. <source>Psychol. Bull</source>. <volume>85</volume>, <fpage>845</fpage>&#x02013;<lpage>857</lpage>. <pub-id pub-id-type="doi">10.1037/0033-2909.85.4.845</pub-id></citation>
</ref>
<ref id="B13">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname> <given-names>J. A.</given-names></name> <name><surname>Murphy</surname> <given-names>N. A.</given-names></name> <name><surname>Mast</surname> <given-names>M. S.</given-names></name></person-group> (<year>2006</year>). <article-title>Recall of nonverbal cues: exploring a new definition of interpersonal sensitivity</article-title>. <source>J. Nonverbal Behav</source>. <volume>30</volume>, <fpage>141</fpage>&#x02013;<lpage>155</lpage>. <pub-id pub-id-type="doi">10.1007/s10919-006-0013-3</pub-id></citation>
</ref>
<ref id="B14">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname> <given-names>J.-D.</given-names></name> <name><surname>Rees</surname> <given-names>G.</given-names></name></person-group> (<year>2005</year>). <article-title>Predicting the stream of consciousness from activity in human visual cortex</article-title>. <source>Curr. Biol</source>. <volume>15</volume>, <fpage>1301</fpage>&#x02013;<lpage>1307</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2005.06.026</pub-id><pub-id pub-id-type="pmid">16051174</pub-id></citation>
</ref>
<ref id="B15">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname> <given-names>J.-D.</given-names></name> <name><surname>Rees</surname> <given-names>G.</given-names></name></person-group> (<year>2006</year>). <article-title>Decoding mental states from brain activity in humans</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>7</volume>, <fpage>523</fpage>&#x02013;<lpage>534</lpage>. <pub-id pub-id-type="doi">10.1038/nrn1931</pub-id><pub-id pub-id-type="pmid">16791142</pub-id></citation>
</ref>
<ref id="B16">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname> <given-names>J.-D.</given-names></name> <name><surname>Sakai</surname> <given-names>K.</given-names></name> <name><surname>Rees</surname> <given-names>G.</given-names></name> <name><surname>Gilbert</surname> <given-names>S.</given-names></name> <name><surname>Frith</surname> <given-names>C.</given-names></name> <name><surname>Passingham</surname> <given-names>R. E.</given-names></name></person-group> (<year>2007</year>). <article-title>Reading hidden intentions in the human brain</article-title>. <source>Curr. Biol</source>. <volume>17</volume>, <fpage>323</fpage>&#x02013;<lpage>328</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2006.11.072</pub-id><pub-id pub-id-type="pmid">17291759</pub-id></citation>
</ref>
<ref id="B17">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname> <given-names>A. P.</given-names></name> <name><surname>Blair</surname> <given-names>R. C.</given-names></name> <name><surname>Watson</surname> <given-names>J. D.</given-names></name> <name><surname>Ford</surname> <given-names>I.</given-names></name></person-group> (<year>1996</year>). <article-title>Nonparametric analysis of statistic images from functional mapping experiments</article-title>. <source>J. Cereb. Blood flow and Metab</source>. <volume>16</volume>, <fpage>7</fpage>&#x02013;<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1097/00004647-199601000-00002</pub-id><pub-id pub-id-type="pmid">8530558</pub-id></citation>
</ref>
<ref id="B18">
<citation citation-type="book"><person-group person-group-type="author"><name><surname>Huettel</surname> <given-names>S. A.</given-names></name> <name><surname>Song</surname> <given-names>A. W.</given-names></name> <name><surname>McCarthy</surname> <given-names>G.</given-names></name></person-group> (<year>2008</year>). <source>Functional Magnetic Resonance Imaging, 2nd Edn</source>. <publisher-loc>Massachusetts</publisher-loc>: <publisher-name>Sinauer Associates</publisher-name> <fpage>510</fpage>.</citation>
</ref>
<ref id="B19">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Junger</surname> <given-names>J.</given-names></name> <name><surname>Pauly</surname> <given-names>K.</given-names></name> <name><surname>Br&#x000F6;hr</surname> <given-names>S.</given-names></name> <name><surname>Birkholz</surname> <given-names>P.</given-names></name> <name><surname>Neuschaefer-Rube</surname> <given-names>C.</given-names></name> <name><surname>Kohler</surname> <given-names>C.</given-names></name> <etal/></person-group>. (<year>2013</year>). <article-title>Sex matters: neural correlates of voice gender perception</article-title>. <source>Neuroimage</source> <volume>79</volume>, <fpage>275</fpage>&#x02013;<lpage>287</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.105</pub-id><pub-id pub-id-type="pmid">23660030</pub-id></citation>
</ref>
<ref id="B20">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kotz</surname> <given-names>S. A.</given-names></name> <name><surname>Kalberlah</surname> <given-names>C.</given-names></name> <name><surname>Bahlmann</surname> <given-names>J.</given-names></name> <name><surname>Friederici</surname> <given-names>A. D.</given-names></name> <name><surname>Haynes</surname> <given-names>J.-D.</given-names></name></person-group> (<year>2013</year>). <article-title>Predicting vocal emotion expressions from the human brain</article-title>. <source>Hum. Brain Mapp</source>. <volume>34</volume>, <fpage>1971</fpage>. <pub-id pub-id-type="doi">10.1002/hbm.22041</pub-id><pub-id pub-id-type="pmid">22371367</pub-id></citation>
</ref>
<ref id="B21">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kreifelts</surname> <given-names>B.</given-names></name> <name><surname>Ethofer</surname> <given-names>T.</given-names></name> <name><surname>Shiozawa</surname> <given-names>T.</given-names></name> <name><surname>Grodd</surname> <given-names>W.</given-names></name> <name><surname>Wildgruber</surname> <given-names>D.</given-names></name></person-group> (<year>2009</year>). <article-title>Cerebral representation of non-verbal emotional perception: fMRI reveals audiovisual integration area between voice- and face-sensitive regions in the superior temporal sulcus</article-title>. <source>Neuropsychologia</source> <volume>47</volume>, <fpage>3059</fpage>&#x02013;<lpage>3066</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2009.07.001</pub-id><pub-id pub-id-type="pmid">19596021</pub-id></citation>
</ref>
<ref id="B22">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname> <given-names>N.</given-names></name> <name><surname>Goebel</surname> <given-names>R.</given-names></name> <name><surname>Bandettini</surname> <given-names>P.</given-names></name></person-group> (<year>2006</year>). <article-title>Information-based functional brain mapping</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A</source>. <volume>103</volume>, <fpage>3863</fpage>&#x02013;<lpage>3868</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id><pub-id pub-id-type="pmid">16537458</pub-id></citation>
</ref>
<ref id="B23">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Latinus</surname> <given-names>M.</given-names></name> <name><surname>Crabbe</surname> <given-names>F.</given-names></name> <name><surname>Belin</surname> <given-names>P.</given-names></name></person-group> (<year>2011</year>). <article-title>Learning-induced changes in the cerebral processing of voice identity</article-title>. <source>Cereb. Cortex</source> <volume>21</volume>, <fpage>2820</fpage>&#x02013;<lpage>2828</lpage>. <pub-id pub-id-type="doi">10.1093/cercor/bhr077</pub-id><pub-id pub-id-type="pmid">21531779</pub-id></citation>
</ref>
<ref id="B23a">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Lattner</surname> <given-names>S.</given-names></name> <name><surname>Meyer</surname> <given-names>M. E.</given-names></name> <name><surname>Friederici</surname> <given-names>A. D.</given-names></name></person-group> (<year>2005</year>). <article-title>Voice perception: sex, pitch, and the right hemisphere</article-title>. <source>Hum. Brain Mapp</source>. <volume>24</volume>, <fpage>11</fpage>&#x02013;<lpage>20</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20065</pub-id><pub-id pub-id-type="pmid">15593269</pub-id></citation>
</ref>
<ref id="B24">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname> <given-names>M.</given-names></name> <name><surname>Bandettini</surname> <given-names>P. A.</given-names></name> <name><surname>Kriegskorte</surname> <given-names>N.</given-names></name></person-group> (<year>2009</year>). <article-title>Revealing representational content with pattern-information fMRIan introductory guide</article-title>. <source>Scan</source> <volume>4</volume>, <fpage>1</fpage>&#x02013;<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1093/scan/nsn044</pub-id><pub-id pub-id-type="pmid">19151374</pub-id></citation>
</ref>
<ref id="B25">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname> <given-names>T. E.</given-names></name> <name><surname>Holmes</surname> <given-names>A. P.</given-names></name></person-group> (<year>2002</year>). <article-title>Nonparametric permutation tests for functional neuroimaging: a primer with examples</article-title>. <source>Hum. Brain Mapp</source>. <volume>15</volume>, <fpage>1</fpage>&#x02013;<lpage>25</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id><pub-id pub-id-type="pmid">11747097</pub-id></citation>
</ref>
<ref id="B26">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname> <given-names>K. A.</given-names></name> <name><surname>Polyn</surname> <given-names>S. M.</given-names></name> <name><surname>Detre</surname> <given-names>G. J.</given-names></name> <name><surname>Haxby</surname> <given-names>J. V.</given-names></name></person-group> (<year>2006</year>). <article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title>. <source>Trends Cogn. Sci</source>. <volume>10</volume>, <fpage>424</fpage>&#x02013;<lpage>430</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2006.07.005</pub-id><pub-id pub-id-type="pmid">16899397</pub-id></citation>
</ref>
<ref id="B27">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Sander</surname> <given-names>K.</given-names></name> <name><surname>Frome</surname> <given-names>Y.</given-names></name> <name><surname>Scheich</surname> <given-names>H.</given-names></name></person-group> (<year>2007</year>). <article-title>FMRI activations of amygdala, cingulate cortex, and auditory cortex by infant laughing and crying</article-title>. <source>Hum. Brain Mapp</source>. <volume>28</volume>, <fpage>1007</fpage>&#x02013;<lpage>1022</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.20333</pub-id><pub-id pub-id-type="pmid">17358020</pub-id></citation>
</ref>
<ref id="B28">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schirmer</surname> <given-names>A.</given-names></name> <name><surname>Kotz</surname> <given-names>S. A.</given-names></name></person-group> (<year>2006</year>). <article-title>Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing</article-title>. <source>Trends Cogn. Sci</source>. <volume>10</volume>, <fpage>24</fpage>&#x02013;<lpage>30</lpage>. <pub-id pub-id-type="doi">10.1016/j.tics.2005.11.009</pub-id><pub-id pub-id-type="pmid">16321562</pub-id></citation>
</ref>
<ref id="B29">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schirmer</surname> <given-names>A.</given-names></name> <name><surname>Kotz</surname> <given-names>S. A.</given-names></name> <name><surname>Friederici</surname> <given-names>A. D.</given-names></name></person-group> (<year>2002</year>). <article-title>Sex differentiates the role of emotional prosody during word processing</article-title>. <source>Cogn. Brain Res</source>. <volume>14</volume>, <fpage>228</fpage>&#x02013;<lpage>223</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(02)00108-8</pub-id><pub-id pub-id-type="pmid">12067695</pub-id></citation>
</ref>
<ref id="B30">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schirmer</surname> <given-names>A.</given-names></name> <name><surname>Simpson</surname> <given-names>E.</given-names></name> <name><surname>Escoffier</surname> <given-names>N.</given-names></name></person-group> (<year>2007</year>). <article-title>Listen up! Processing of intensity change differs for vocal and nonvocal sounds</article-title>. <source>Brain Res</source>. <volume>1176</volume>, <fpage>103</fpage>&#x02013;<lpage>112</lpage>. <pub-id pub-id-type="doi">10.1016/j.brainres.2007.08.008</pub-id><pub-id pub-id-type="pmid">17900543</pub-id></citation>
</ref>
<ref id="B31">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schirmer</surname> <given-names>A.</given-names></name> <name><surname>Striano</surname> <given-names>T.</given-names></name> <name><surname>Friederici</surname> <given-names>A. D.</given-names></name></person-group> (<year>2005</year>). <article-title>Sex differences in the preattentive processing of vocal emotional expressions</article-title>. <source>Neuroreport</source> <volume>16</volume>, <fpage>635</fpage>&#x02013;<lpage>639</lpage>. <pub-id pub-id-type="doi">10.1097/00001756-200504250-00024</pub-id><pub-id pub-id-type="pmid">15812323</pub-id></citation>
</ref>
<ref id="B32">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Schirmer</surname> <given-names>A.</given-names></name> <name><surname>Zysset</surname> <given-names>S.</given-names></name> <name><surname>Kotz</surname> <given-names>S. A.</given-names></name> <name><surname>Yves von Cramon</surname> <given-names>D.</given-names></name></person-group> (<year>2004</year>). <article-title>Gender differences in the activation of inferior frontal cortex during emotional speech perception</article-title>. <source>Neuroimage</source> <volume>21</volume>, <fpage>1114</fpage>&#x02013;<lpage>1123</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2003.10.048</pub-id><pub-id pub-id-type="pmid">15006679</pub-id></citation>
</ref>
<ref id="B33">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>S. K.</given-names></name> <name><surname>Johnsrude</surname> <given-names>I. S.</given-names></name></person-group> (<year>2003</year>). <article-title>The neuroanatomical and functional organization of speech perception</article-title>. <source>Trends Neurosci</source>. <volume>26</volume>, <fpage>100</fpage>&#x02013;<lpage>107</lpage>. <pub-id pub-id-type="doi">10.1016/S0166-2236(02)00037-1</pub-id><pub-id pub-id-type="pmid">12536133</pub-id></citation>
</ref>
<ref id="B34">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Seifritz</surname> <given-names>E.</given-names></name> <name><surname>Esposito</surname> <given-names>F.</given-names></name> <name><surname>Neuhoff</surname> <given-names>J. G.</given-names></name> <name><surname>L&#x000FC;thi</surname> <given-names>A.</given-names></name> <name><surname>Mustovic</surname> <given-names>H.</given-names></name> <name><surname>Dammann</surname> <given-names>G.</given-names></name> <etal/></person-group>. (<year>2003</year>). <article-title>Differential sex-independent amygdala response to infant crying and laughing in parents versus nonparents</article-title>. <source>Biol. Psychiatry</source> <volume>54</volume>, <fpage>1367</fpage>&#x02013;<lpage>1375</lpage>. <pub-id pub-id-type="doi">10.1016/S0006-3223(03)00697-8</pub-id><pub-id pub-id-type="pmid">14675800</pub-id></citation>
</ref>
<ref id="B35">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Shaywitz</surname> <given-names>B. A.</given-names></name> <name><surname>Shaywitz</surname> <given-names>S. E.</given-names></name> <name><surname>Pugh</surname> <given-names>K. R.</given-names></name> <name><surname>Constable</surname> <given-names>R. T.</given-names></name> <name><surname>Skudlarski</surname> <given-names>P.</given-names></name> <name><surname>Fulbright</surname> <given-names>R. K.</given-names></name> <etal/></person-group>. (<year>1995</year>). <article-title>Sex differences in the functional organization of the brain for language</article-title>. <source>Nature</source> <volume>373</volume>, <fpage>607</fpage>&#x02013;<lpage>609</lpage>. <pub-id pub-id-type="doi">10.1038/373607a0</pub-id><pub-id pub-id-type="pmid">7854416</pub-id></citation>
</ref>
<ref id="B36">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Skuk</surname> <given-names>V. G.</given-names></name> <name><surname>Schweinberger</surname> <given-names>S. R.</given-names></name></person-group> (<year>2013</year>). <article-title>Gender differences in familiar voice identification</article-title>. <source>Hear. Res</source>. <volume>296</volume>, <fpage>131</fpage>&#x02013;<lpage>140</lpage>. <pub-id pub-id-type="doi">10.1016/j.heares.2012.11.004</pub-id><pub-id pub-id-type="pmid">23168357</pub-id></citation>
</ref>
<ref id="B37">
<citation citation-type="journal"><person-group person-group-type="author"><name><surname>Von Kriegstein</surname> <given-names>K.</given-names></name> <name><surname>Eger</surname> <given-names>E.</given-names></name> <name><surname>Kleinschmidt</surname> <given-names>A.</given-names></name> <name><surname>Giraud</surname> <given-names>A. L.</given-names></name></person-group> (<year>2003</year>). <article-title>Modulation of neural responses to speech by directing attention to voices or verbal content</article-title>. <source>Cogn. Brain Res</source>. <volume>17</volume>, <fpage>48</fpage>&#x02013;<lpage>55</lpage>. <pub-id pub-id-type="doi">10.1016/S0926-6410(03)00079-X</pub-id><pub-id pub-id-type="pmid">12763191</pub-id></citation>
</ref>
</ref-list>
</back>
</article>